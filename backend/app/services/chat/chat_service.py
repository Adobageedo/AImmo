"""
Chat Service - Phase 5 Chat MVP
Chat avec RAG, streaming et g√©n√©ration LLM
"""

import json
import time
from datetime import datetime
from typing import List, Dict, Any, Optional, AsyncGenerator
from uuid import UUID, uuid4

from openai import OpenAI

from app.core.config import settings
from app.schemas.chat import (
    MessageRole,
    ChatMode,
    ChatRequest,
    ChatResponse,
    Message,
    MessageCreate,
    Citation,
    Conversation,
    ConversationCreate,
    ConversationWithMessages,
    StreamChunk,
    LeasesSummaryRequest,
    LeasesSummaryResponse,
    PropertyComparisonRequest,
    PropertyComparisonResponse,
    TableGenerationRequest,
    TableGenerationResponse,
    ExportFormat,
    ExportRequest,
    ExportResponse,
    PromptSuggestion,
    PromptCategory,
)
from app.schemas.rag import SourceType, RAGSearchRequest, RAGSearchResult
from app.services.rag.rag_service import search_chunks


# ============================================
# Client OpenAI
# ============================================

def get_openai_client() -> OpenAI:
    """Obtenir le client OpenAI"""
    return OpenAI(api_key=settings.OPENAI_API_KEY)


# ============================================
# Prompts Syst√®me
# ============================================

SYSTEM_PROMPT = """Tu es AImmo, un assistant IA sp√©cialis√© dans la gestion immobili√®re.

Tu aides les gestionnaires immobiliers et propri√©taires √† :
- Analyser et r√©sumer des baux et contrats
- Comparer des biens immobiliers
- G√©n√©rer des rapports et tableaux
- R√©pondre √† des questions sur leurs donn√©es

Instructions :
1. R√©ponds toujours en fran√ßais
2. Sois pr√©cis et concis
3. Cite tes sources quand tu utilises des informations des documents
4. Si tu ne connais pas la r√©ponse, dis-le clairement
5. Formate tes r√©ponses avec du Markdown quand appropri√©

{context}
"""

RAG_CONTEXT_TEMPLATE = """
Voici des informations pertinentes tir√©es des documents de l'utilisateur :

{chunks}

Utilise ces informations pour r√©pondre √† la question. Cite les sources en utilisant [Source: titre du document].
"""


# ============================================
# G√©n√©ration de Citations
# ============================================

def create_citations_from_results(
    results: List[RAGSearchResult],
    max_citations: int = 5
) -> List[Citation]:
    """Cr√©e des citations √† partir des r√©sultats RAG"""
    citations = []
    
    for i, result in enumerate(results[:max_citations]):
        citation = Citation(
            id=uuid4(),
            chunk_id=result.chunk_id,
            document_id=result.document_id,
            document_title=result.metadata.source_title or f"Document {i+1}",
            content_preview=result.content[:200] + "..." if len(result.content) > 200 else result.content,
            page_number=result.metadata.page_number,
            source_type=result.source_type,
            relevance_score=result.score,
            url=f"/dashboard/documents/{result.document_id}",
        )
        citations.append(citation)
    
    return citations


# ============================================
# Construction du Contexte RAG
# ============================================

def build_rag_context(results: List[RAGSearchResult]) -> str:
    """Construit le contexte RAG pour le prompt"""
    if not results:
        return ""
    
    chunks_text = []
    for i, result in enumerate(results):
        source_info = f"[Source {i+1}: {result.metadata.source_title or 'Document'}]"
        if result.metadata.page_number:
            source_info += f" (Page {result.metadata.page_number})"
        
        chunks_text.append(f"{source_info}\n{result.content}\n")
    
    context = RAG_CONTEXT_TEMPLATE.format(chunks="\n".join(chunks_text))
    return context


# ============================================
# Chat Principal
# ============================================

async def process_chat(
    request: ChatRequest,
    organization_id: UUID,
    supabase_client: Any,
) -> ChatResponse:
    """Traite une requ√™te de chat"""
    start_time = time.time()
    
    # 1. Recherche RAG si n√©cessaire
    rag_results: List[RAGSearchResult] = []
    citations: List[Citation] = []
    
    if request.mode in [ChatMode.RAG_ONLY, ChatMode.RAG_ENHANCED]:
        rag_request = RAGSearchRequest(
            organization_id=organization_id,
            query=request.message,
            source_types=request.source_types,
            document_ids=request.document_ids,
            lease_ids=request.lease_ids,
            property_ids=request.property_ids,
            limit=10,
            min_score=0.5,
        )
        
        rag_response = await search_chunks(rag_request)
        rag_results = rag_response.results
        
        # Toujours cr√©er les citations si des r√©sultats existent
        citations = create_citations_from_results(rag_results, 5) if rag_results else []
    
    # 2. Mode RAG Only - retourner juste les r√©sultats
    if request.mode == ChatMode.RAG_ONLY:
        content = _format_rag_only_response(rag_results)
        
        message = Message(
            id=uuid4(),
            conversation_id=request.conversation_id,
            role=MessageRole.ASSISTANT,
            content=content,
            citations=citations,
            created_at=datetime.utcnow(),
        )
        
        return ChatResponse(
            message=message,
            citations=citations,
            rag_results=rag_results,
            processing_time_ms=int((time.time() - start_time) * 1000),
        )
    
    # 3. G√©n√©ration LLM avec contexte RAG
    context = build_rag_context(rag_results) if rag_results else ""
    system_prompt = SYSTEM_PROMPT.format(context=context)
    
    # R√©cup√©rer l'historique de la conversation
    history = await _get_conversation_history(request.conversation_id, supabase_client)
    
    # Construire les messages
    messages = [{"role": "system", "content": system_prompt}]
    
    for msg in history[-10:]:  # Limiter √† 10 derniers messages
        messages.append({
            "role": msg["role"],
            "content": msg["content"],
        })
    
    messages.append({"role": "user", "content": request.message})
    
    # Appeler OpenAI
    client = get_openai_client()
    
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages,
        temperature=0.7,
        max_tokens=2000,
    )
    
    assistant_content = response.choices[0].message.content
    
    # Cr√©er le message de r√©ponse
    message = Message(
        id=uuid4(),
        conversation_id=request.conversation_id,
        role=MessageRole.ASSISTANT,
        content=assistant_content,
        citations=citations,
        metadata={"model": "gpt-4o-mini", "tokens": response.usage.total_tokens},
        created_at=datetime.utcnow(),
    )
    
    # Sauvegarder les messages dans Supabase
    await _save_messages(
        request.conversation_id,
        request.message,
        assistant_content,
        citations,
        supabase_client,
    )
    
    return ChatResponse(
        message=message,
        citations=citations,
        rag_results=rag_results if rag_results else None,
        processing_time_ms=int((time.time() - start_time) * 1000),
    )


async def process_chat_stream(
    request: ChatRequest,
    organization_id: UUID,
    supabase_client: Any,
) -> AsyncGenerator[StreamChunk, None]:
    """Traite une requ√™te de chat en streaming"""
    
    # 1. Recherche RAG
    rag_results: List[RAGSearchResult] = []
    citations: List[Citation] = []
    
    if request.mode in [ChatMode.RAG_ONLY, ChatMode.RAG_ENHANCED]:
        rag_request = RAGSearchRequest(
            organization_id=organization_id,
            query=request.message,
            source_types=request.source_types,
            document_ids=request.document_ids,
            limit=10,
            min_score=0.5,
        )
        
        rag_response = await search_chunks(rag_request)
        rag_results = rag_response.results
        
        # Toujours cr√©er les citations si des r√©sultats existent
        citations = create_citations_from_results(rag_results, 5) if rag_results else []
        
        # Envoyer les citations d'abord
        for citation in citations:
            yield StreamChunk(type="citation", citation=citation)
    
    # 2. Mode RAG Only
    if request.mode == ChatMode.RAG_ONLY:
        content = _format_rag_only_response(rag_results)
        yield StreamChunk(type="content", content=content)
        yield StreamChunk(type="done")
        return
    
    # 3. Streaming LLM
    context = build_rag_context(rag_results) if rag_results else ""
    system_prompt = SYSTEM_PROMPT.format(context=context)
    
    history = await _get_conversation_history(request.conversation_id, supabase_client)
    
    messages = [{"role": "system", "content": system_prompt}]
    for msg in history[-10:]:
        messages.append({"role": msg["role"], "content": msg["content"]})
    messages.append({"role": "user", "content": request.message})
    
    client = get_openai_client()
    
    try:
        stream = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            temperature=0.7,
            max_tokens=2000,
            stream=True,
        )
        
        full_content = ""
        for chunk in stream:
            if chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                full_content += content
                yield StreamChunk(type="content", content=content)
        
        # Sauvegarder les messages
        await _save_messages(
            request.conversation_id,
            request.message,
            full_content,
            citations,
            supabase_client,
        )
        
        yield StreamChunk(type="done")
        
    except Exception as e:
        yield StreamChunk(type="error", error=str(e))


def _format_rag_only_response(results: List[RAGSearchResult]) -> str:
    """Formate la r√©ponse en mode RAG only"""
    if not results:
        return "Aucun r√©sultat trouv√© dans vos documents."
    
    response_parts = [f"**{len(results)} r√©sultats trouv√©s :**\n"]
    
    for i, result in enumerate(results):
        title = result.metadata.source_title or "Document"
        score_pct = int(result.score * 100)
        
        response_parts.append(f"\n### {i+1}. {title} ({score_pct}% pertinent)\n")
        response_parts.append(f"{result.content[:500]}{'...' if len(result.content) > 500 else ''}\n")
        
        if result.semantic_tags:
            tags = ", ".join(result.semantic_tags[:5])
            response_parts.append(f"*Tags: {tags}*\n")
    
    return "".join(response_parts)


async def _get_conversation_history(
    conversation_id: UUID,
    supabase_client: Any,
) -> List[Dict[str, Any]]:
    """R√©cup√®re l'historique d'une conversation"""
    try:
        response = supabase_client.table("messages").select("*").eq(
            "conversation_id", str(conversation_id)
        ).order("created_at").execute()
        
        return response.data if response.data else []
    except Exception:
        return []


async def _save_messages(
    conversation_id: UUID,
    user_message: str,
    assistant_message: str,
    citations: List[Citation],
    supabase_client: Any,
):
    """Sauvegarde les messages dans Supabase"""
    try:
        # Message utilisateur
        supabase_client.table("messages").insert({
            "id": str(uuid4()),
            "conversation_id": str(conversation_id),
            "role": "user",
            "content": user_message,
            "created_at": datetime.utcnow().isoformat(),
        }).execute()
        
        # Message assistant
        citations_data = [c.model_dump(mode="json") for c in citations]
        supabase_client.table("messages").insert({
            "id": str(uuid4()),
            "conversation_id": str(conversation_id),
            "role": "assistant",
            "content": assistant_message,
            "metadata": {"citations": citations_data},
            "created_at": datetime.utcnow().isoformat(),
        }).execute()
        
        # Mettre √† jour la conversation
        supabase_client.table("conversations").update({
            "updated_at": datetime.utcnow().isoformat(),
        }).eq("id", str(conversation_id)).execute()
        
    except Exception as e:
        print(f"Error saving messages: {e}")


# ============================================
# Capacit√©s Sp√©ciales
# ============================================

async def summarize_lease(
    request: LeasesSummaryRequest,
    organization_id: UUID,
    supabase_client: Any,
) -> LeasesSummaryResponse:
    """G√©n√®re un r√©sum√© de bail"""
    
    # Rechercher les chunks du bail
    rag_request = RAGSearchRequest(
        organization_id=organization_id,
        query="bail contrat location loyer charges dates conditions",
        lease_ids=[request.lease_id],
        source_types=[SourceType.LEASES, SourceType.DOCUMENTS],
        limit=20,
        min_score=0.3,
    )
    
    rag_response = await search_chunks(rag_request)
    context = build_rag_context(rag_response.results)
    
    prompt = f"""Analyse ce bail et g√©n√®re un r√©sum√© structur√©.

{context}

G√©n√®re un r√©sum√© JSON avec les sections suivantes:
- summary: r√©sum√© g√©n√©ral (2-3 phrases)
- key_dates: dates importantes (d√©but, fin, renouvellement, pr√©avis)
- financials: informations financi√®res (loyer, charges, d√©p√¥t)
- important_clauses: clauses importantes √† noter

R√©ponds UNIQUEMENT avec du JSON valide."""

    client = get_openai_client()
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3,
        response_format={"type": "json_object"},
    )
    
    result = json.loads(response.choices[0].message.content)
    
    return LeasesSummaryResponse(
        lease_id=request.lease_id,
        summary=result.get("summary", ""),
        key_dates=result.get("key_dates") if request.include_key_dates else None,
        financials=result.get("financials") if request.include_financials else None,
        important_clauses=result.get("important_clauses") if request.include_clauses else None,
    )


async def compare_properties(
    request: PropertyComparisonRequest,
    organization_id: UUID,
    supabase_client: Any,
) -> PropertyComparisonResponse:
    """Compare plusieurs biens immobiliers"""
    
    properties_data = []
    
    for property_id in request.property_ids:
        # Rechercher les infos du bien
        rag_request = RAGSearchRequest(
            organization_id=organization_id,
            query="surface prix loyer adresse caract√©ristiques √©tat",
            property_ids=[property_id],
            limit=10,
            min_score=0.3,
        )
        
        rag_response = await search_chunks(rag_request)
        
        property_info = {
            "id": str(property_id),
            "chunks": [r.content for r in rag_response.results],
        }
        properties_data.append(property_info)
    
    # G√©n√©rer la comparaison
    prompt = f"""Compare ces biens immobiliers:

{json.dumps(properties_data, indent=2, ensure_ascii=False)}

Crit√®res de comparaison: {', '.join(request.criteria or ['surface', 'prix', 'localisation', '√©tat'])}

G√©n√®re un JSON avec:
- properties: liste des biens avec leurs caract√©ristiques extraites
- comparison_table: tableau comparatif avec une cl√© par crit√®re et une liste de valeurs par bien
- analysis: analyse comparative (avantages/inconv√©nients de chaque bien)

R√©ponds UNIQUEMENT avec du JSON valide."""

    client = get_openai_client()
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3,
        response_format={"type": "json_object"},
    )
    
    result = json.loads(response.choices[0].message.content)
    
    return PropertyComparisonResponse(
        properties=result.get("properties", []),
        comparison_table=result.get("comparison_table", {}),
        analysis=result.get("analysis", ""),
    )


async def generate_table(
    request: TableGenerationRequest,
    organization_id: UUID,
    supabase_client: Any,
) -> TableGenerationResponse:
    """G√©n√®re un tableau √† partir des donn√©es"""
    
    # Rechercher les donn√©es pertinentes
    rag_request = RAGSearchRequest(
        organization_id=organization_id,
        query=request.query,
        source_types=request.source_types,
        limit=20,
        min_score=0.4,
    )
    
    rag_response = await search_chunks(rag_request)
    context = build_rag_context(rag_response.results)
    
    columns_hint = f"Colonnes souhait√©es: {', '.join(request.columns)}" if request.columns else ""
    
    prompt = f"""√Ä partir de ces donn√©es, g√©n√®re un tableau.

{context}

Requ√™te: {request.query}
{columns_hint}

G√©n√®re un JSON avec:
- headers: liste des en-t√™tes de colonnes
- rows: liste de lignes (chaque ligne est une liste de valeurs)
- summary: r√©sum√© optionnel des donn√©es

R√©ponds UNIQUEMENT avec du JSON valide."""

    client = get_openai_client()
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3,
        response_format={"type": "json_object"},
    )
    
    result = json.loads(response.choices[0].message.content)
    
    return TableGenerationResponse(
        headers=result.get("headers", []),
        rows=result.get("rows", []),
        summary=result.get("summary"),
    )


# ============================================
# Suggestions de Prompts
# ============================================

def get_prompt_suggestions() -> List[PromptSuggestion]:
    """Retourne les suggestions de prompts"""
    return [
        PromptSuggestion(
            id="lease_summary",
            category=PromptCategory.LEASE_ANALYSIS,
            title="R√©sumer un bail",
            prompt="R√©sume le bail de [locataire] avec les dates cl√©s et les conditions de paiement",
            icon="üìã",
        ),
        PromptSuggestion(
            id="lease_expiry",
            category=PromptCategory.LEASE_ANALYSIS,
            title="Baux expirants",
            prompt="Quels baux expirent dans les 3 prochains mois ?",
            icon="‚è∞",
        ),
        PromptSuggestion(
            id="compare_properties",
            category=PromptCategory.PROPERTY_COMPARISON,
            title="Comparer des biens",
            prompt="Compare les biens [A] et [B] en termes de surface, prix et localisation",
            icon="‚öñÔ∏è",
        ),
        PromptSuggestion(
            id="vacancy_rate",
            category=PromptCategory.FINANCIAL_REPORT,
            title="Taux de vacance",
            prompt="Quel est le taux de vacance actuel de mon portefeuille ?",
            icon="üìä",
        ),
        PromptSuggestion(
            id="rent_table",
            category=PromptCategory.FINANCIAL_REPORT,
            title="Tableau des loyers",
            prompt="G√©n√®re un tableau r√©capitulatif de tous les loyers par bien",
            icon="üìà",
        ),
        PromptSuggestion(
            id="unpaid_rent",
            category=PromptCategory.FINANCIAL_REPORT,
            title="Impay√©s",
            prompt="Liste les loyers impay√©s de ce mois",
            icon="üí∞",
        ),
        PromptSuggestion(
            id="search_docs",
            category=PromptCategory.GENERAL,
            title="Rechercher dans les documents",
            prompt="Trouve les documents mentionnant [terme]",
            icon="üîç",
        ),
        PromptSuggestion(
            id="index_analysis",
            category=PromptCategory.LEASE_ANALYSIS,
            title="R√©vision des loyers",
            prompt="Quels baux sont √©ligibles √† une r√©vision de loyer ?",
            icon="üìÖ",
        ),
    ]
